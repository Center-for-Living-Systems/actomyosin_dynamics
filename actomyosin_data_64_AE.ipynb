{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 22:31:02.992660: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-21 22:31:02.992711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-21 22:31:02.993357: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-21 22:31:02.998842: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils import actomyosin_data, plot64\n",
    "from model.autoencoder_64 import AE, VAE, CVAE\n",
    "from train_utils.autoencoder import AETrain, VAETrain, CVAETrain\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave, imread\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/fit/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/net/projects/CLS/actomyosin_dynamics/data/LifeAct-NMY2-GFP_patchsize_64_p95_all/NMY2_wt'\n",
    "patch_size = 64\n",
    "\n",
    "filenames = [x for x in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, x)) and ('.tif' in x)]\n",
    "\n",
    "num_of_samples = len(filenames)\n",
    "\n",
    "train_images = np.zeros([num_of_samples, patch_size, patch_size])\n",
    "train_labels= np.zeros([num_of_samples])\n",
    "\n",
    "for filename_ind in range(num_of_samples):\n",
    "    filename = filenames[filename_ind] \n",
    "             \n",
    "    train_img = tifffile.imread(os.path.join(data_dir,filename))\n",
    "    small_train_img = train_img/((train_img.max()))\n",
    "    train_images[filename_ind,:,:] = small_train_img\n",
    "    train_labels[filename_ind] = int(filename[1:5]+filename[6:10]+filename[11:15])\n",
    "\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 64, 64, 1).astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 3000\n",
    "batch_size = 600\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "train_dataset_image = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)\n",
    "train_dataset_label = tf.data.Dataset.from_tensor_slices(train_labels).batch(BATCH_SIZE)\n",
    "train_dataset = tf.data.Dataset.zip((train_dataset_image, train_dataset_label)).shuffle(TRAIN_BUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_type = 'AE'\n",
    "latent_dim = 32\n",
    "num_epochs = 200\n",
    "learn_rate = 0.001\n",
    "\n",
    "model = AE(latent_dim, net_type='conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learn_rate)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learn_rate' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learn_rate)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t = time.time()\n",
    "    last_loss = 0\n",
    "    for train_x, _ in train_dataset:\n",
    "        gradients, loss = AETrain.compute_gradients(model, train_x)\n",
    "        AETrain.apply_gradients(optimizer, gradients, model.trainable_variables)\n",
    "        last_loss = loss\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}, Loss: {}, Remaining Time at This Epoch: {:.2f}'.format(\n",
    "            epoch, last_loss, time.time() - t\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n = 10\n",
    "\n",
    "fig_dist = plt.figure(figsize=(8, 8))\n",
    "ax_dist = fig_dist.add_subplot(111)\n",
    "\n",
    "flag_sample = 1\n",
    "\n",
    "for x_input, y_input in train_dataset:\n",
    "    if flag_sample == 1:        \n",
    "        x_input_sample, y_input_sample = map(lambda x: x[:n], (x_input, y_input))\n",
    "        z = model.encode(x_input_sample).numpy()\n",
    "\n",
    "        fig1, axarr1 = plt.subplots(2, n, figsize=(n, 2))\n",
    "        x_input_sample = x_input_sample.numpy().reshape([n, 64, 64])\n",
    "        x_output = model.decode(z).numpy().reshape([n, 64, 64])\n",
    "\n",
    "        for i in range(n):\n",
    "            axarr1[0, i].axis('off')\n",
    "            axarr1[1, i].axis('off')\n",
    "            axarr1[0, i].imshow(x_input_sample[i],cmap=plt.cm.gray)\n",
    "            axarr1[1, i].imshow(x_output[i],cmap=plt.cm.gray)\n",
    "\n",
    "        fig1.savefig(\"results/actomyosin_64_AE_reconstruction.png\")\n",
    "        \n",
    "        z = model.encode(x_input)\n",
    "        Z_array = z.numpy()\n",
    "        Label_array = y_input\n",
    "        flag_sample = 0\n",
    "    else:\n",
    "        z = model.encode(x_input)\n",
    "        Z_array = np.concatenate((Z_array,z.numpy()), axis=0)\n",
    "        Label_array = np.concatenate((Label_array,y_input.numpy()), axis=0)\n",
    "        \n",
    "    \n",
    "ax_dist.scatter(Z_array[:,0], Z_array[:,1], color='blue',s=0.5)\n",
    "    \n",
    "fig_dist.savefig(\"results/AE_distribution.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dist = plt.figure(figsize=(8, 8))\n",
    "ax_dist = fig_dist.add_subplot(111)\n",
    "ax_dist.scatter(Z_array[:,0], Z_array[:,1], color='blue',s=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import warnings\n",
    "from itertools import cycle, islice\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 500\n",
    "seed = 30\n",
    "rng = np.random.RandomState(seed)\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "no_structure = rng.rand(n_samples, 2), None\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 3,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 7,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "    \"allow_single_cluster\": True,\n",
    "    \"hdbscan_min_cluster_size\": 15,\n",
    "    \"hdbscan_min_samples\": 3,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    \n",
    "    params = default_base.copy()\n",
    "\n",
    "    params.update(algo_params)\n",
    "\n",
    "    # X0 = Z_array[:,0]\n",
    "    # X1 = Z_array[:,1]\n",
    "    \n",
    "    # X00 = X0[X0<10]\n",
    "    # X10 = X1[X0<10]\n",
    "    \n",
    "    # X001 = X00[X10<10]\n",
    "    # X101 = X10[X10<10]\n",
    "    \n",
    "    # X = np.concatenate((X001.reshape([len(X001),1]),X101.reshape([len(X001),1])), axis=1)\n",
    "    \n",
    "    X = Z_array\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    \n",
    "    clustering_algorithms = (\n",
    "        # (\"MeanShift\", ms),\n",
    "        (\"Spectral\\nClustering\", spectral),\n",
    "        # (\"MiniBatch\\nKMeans\", two_means),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\"\n",
    "                + \" may not work as expected.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        # plt.xlim(-2.5, 2.5)\n",
    "        # plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.99,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C0_dir = '/net/projects/CLS/actomyosin_dynamics/data/clustering_results/C0'\n",
    "C1_dir = '/net/projects/CLS/actomyosin_dynamics/data/clustering_results/C1'\n",
    "C2_dir = '/net/projects/CLS/actomyosin_dynamics/data/clustering_results/C2'\n",
    "C0_dec_dir = '/net/projects/CLS/actomyosin_dynamics/data/clustering_results/C0_decoded'\n",
    "C1_dec_dir = '/net/projects/CLS/actomyosin_dynamics/data/clustering_results/C1_decoded'\n",
    "C2_dec_dir = '/net/projects/CLS/actomyosin_dynamics/data/clustering_results/C2_decoded'\n",
    "import pandas as pd\n",
    "\n",
    "csv_output = pd.DataFrame(columns=['title', 'labelnumber','cluster_ID'])\n",
    "for x_input, y_input in train_dataset:\n",
    "    z = model.encode(x_input)\n",
    "    x_output = model.decode(z).numpy().reshape([len(y_input), 64, 64])\n",
    "    \n",
    "    # y_pred = algorithm.predict(z.numpy())\n",
    "    for i in range(z.numpy().shape[0]):\n",
    "        input_label = y_input[i]\n",
    "        input_img = x_input[i,:,:,0].numpy().squeeze()\n",
    "        output_img = x_output[i,:,:]\n",
    "        label_str = str(int(input_label)).zfill(12)\n",
    "        ind = np.where(Label_array == input_label)        \n",
    "        pred_ID = y_pred[ind]        \n",
    "        \n",
    "        title = 't'+label_str[0:4]+'x'+label_str[4:8]+'y'+label_str[8:12]+'ps64.tif'\n",
    "        if pred_ID == 0:\n",
    "            tifffile.imsave(os.path.join(C0_dir,title), input_img)\n",
    "            tifffile.imsave(os.path.join(C0_dec_dir,'dec_'+title), output_img)\n",
    "        if pred_ID == 1:\n",
    "            tifffile.imsave(os.path.join(C1_dir,title), input_img)\n",
    "            tifffile.imsave(os.path.join(C1_dec_dir,'dec_'+title), output_img)\n",
    "        if pred_ID == 2:\n",
    "            tifffile.imsave(os.path.join(C2_dir,title), input_img)\n",
    "            tifffile.imsave(os.path.join(C2_dec_dir,'dec_'+title), output_img)\n",
    "            \n",
    "        s = pd.Series([title, input_label,pred_ID],\n",
    "                           index=['title', 'labelnumber','cluster_ID'])\n",
    "        csv_output = csv_output.append(s,ignore_index=True)\n",
    "                            \n",
    "        \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aetf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
