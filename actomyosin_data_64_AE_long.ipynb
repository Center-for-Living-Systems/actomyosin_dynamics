{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from utils import actomyosin_data, plot64\n",
    "from model.autoencoder_64 import AE, VAE, CVAE\n",
    "from train_utils.autoencoder import AETrain, VAETrain, CVAETrain\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave, imread\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/net/projects/CLS/actomyosin_dynamics/data/LifeAct-NMY2-GFP_NMY2_wt_all_ps_64/NMY2_wt'\n",
    "patch_size = 64\n",
    "\n",
    "filenames = [x for x in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, x)) and ('.tif' in x)]\n",
    "\n",
    "num_of_samples = len(filenames)\n",
    "\n",
    "train_images = np.zeros([num_of_samples, patch_size, patch_size])\n",
    "train_labels= np.zeros([num_of_samples])\n",
    "\n",
    "for filename_ind in range(num_of_samples):\n",
    "    filename = filenames[filename_ind] \n",
    "             \n",
    "    train_img = tifffile.imread(os.path.join(data_dir,filename))\n",
    "    small_train_img = train_img/((train_img.max()))\n",
    "    train_images[filename_ind,:,:] = small_train_img\n",
    "    train_labels[filename_ind] = int(filename[1:5]+filename[6:10]+filename[11:15])\n",
    "\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 64, 64, 1).astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 2000    \n",
    "batch_size = 200\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "train_dataset_image = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)\n",
    "train_dataset_label = tf.data.Dataset.from_tensor_slices(train_labels).batch(BATCH_SIZE)\n",
    "train_dataset = tf.data.Dataset.zip((train_dataset_image, train_dataset_label)).shuffle(TRAIN_BUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_type = 'AE'\n",
    "latent_dim = 2\n",
    "num_epochs = 200\n",
    "learn_rate = 0.001\n",
    "net_type = 'simple'\n",
    "\n",
    "model = AE(latent_dim, net_type=net_type)\n",
    "\n",
    "model_ID = ae_type+'_ld'+str(latent_dim)+'_nt'+net_type+'_bs'+ str(batch_size)+'_buf'+str(TRAIN_BUF)\n",
    "\n",
    "model_ID = 'long_' + model_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learn_rate)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t = time.time()\n",
    "    last_loss = 0\n",
    "    for train_x, _ in train_dataset:\n",
    "        gradients, loss = AETrain.compute_gradients(model, train_x)\n",
    "        AETrain.apply_gradients(optimizer, gradients, model.trainable_variables)\n",
    "        last_loss = loss\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {}, Loss: {}, Remaining Time at This Epoch: {:.2f}'.format(\n",
    "            epoch, last_loss, time.time() - t\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n = 10\n",
    "\n",
    "fig_dist = plt.figure(figsize=(8, 8))\n",
    "ax_dist = fig_dist.add_subplot(111)\n",
    "\n",
    "flag_sample = 1\n",
    "\n",
    "for x_input, y_input in train_dataset:\n",
    "    if flag_sample == 1:        \n",
    "        x_input_sample, y_input_sample = map(lambda x: x[:n], (x_input, y_input))\n",
    "        z = model.encode(x_input_sample).numpy()\n",
    "\n",
    "        fig1, axarr1 = plt.subplots(2, n, figsize=(n, 2))\n",
    "        x_input_sample = x_input_sample.numpy().reshape([n, 64, 64])\n",
    "        x_output = model.decode(z).numpy().reshape([n, 64, 64])\n",
    "\n",
    "        for i in range(n):\n",
    "            axarr1[0, i].axis('off')\n",
    "            axarr1[1, i].axis('off')\n",
    "            axarr1[0, i].imshow(x_input_sample[i],cmap=plt.cm.gray)\n",
    "            axarr1[1, i].imshow(x_output[i],cmap=plt.cm.gray)\n",
    "\n",
    "        fig1.savefig(\"results/\"+model_ID+\"_long_reconstruction.png\")\n",
    "        \n",
    "        z = model.encode(x_input)\n",
    "        Z_array = z.numpy()\n",
    "        Label_array = y_input\n",
    "        flag_sample = 0\n",
    "    else:\n",
    "        z = model.encode(x_input)\n",
    "        Z_array = np.concatenate((Z_array,z.numpy()), axis=0)\n",
    "        Label_array = np.concatenate((Label_array,y_input.numpy()), axis=0)\n",
    "        \n",
    "    \n",
    "ax_dist.scatter(Z_array[:,0], Z_array[:,1], color='blue',s=0.5)\n",
    "    \n",
    "fig_dist.savefig(\"results/\"+model_ID+\"_long_distribution.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dist = plt.figure(figsize=(8, 8))\n",
    "ax_dist = fig_dist.add_subplot(111)\n",
    "ax_dist.scatter(Z_array[:,0], Z_array[:,1], color='blue',s=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import warnings\n",
    "from itertools import cycle, islice\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 500\n",
    "seed = 30\n",
    "rng = np.random.RandomState(seed)\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "no_structure = rng.rand(n_samples, 2), None\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 3,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 7,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "    \"allow_single_cluster\": True,\n",
    "    \"hdbscan_min_cluster_size\": 15,\n",
    "    \"hdbscan_min_samples\": 3,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    \n",
    "    params = default_base.copy()\n",
    "\n",
    "    params.update(algo_params)\n",
    "\n",
    "    # X0 = Z_array[:,0]\n",
    "    # X1 = Z_array[:,1]\n",
    "    \n",
    "    # X00 = X0[X0<10]\n",
    "    # X10 = X1[X0<10]\n",
    "    \n",
    "    # X001 = X00[X10<10]\n",
    "    # X101 = X10[X10<10]\n",
    "    \n",
    "    # X = np.concatenate((X001.reshape([len(X001),1]),X101.reshape([len(X001),1])), axis=1)\n",
    "    \n",
    "    X = Z_array\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params[\"n_clusters\"],\n",
    "        eigen_solver=\"arpack\",\n",
    "        affinity=\"nearest_neighbors\",\n",
    "        random_state=params[\"random_state\"],\n",
    "    )\n",
    "    \n",
    "    clustering_algorithms = (\n",
    "        # (\"MeanShift\", ms),\n",
    "        (\"Spectral\\nClustering\", spectral),\n",
    "        # (\"MiniBatch\\nKMeans\", two_means),\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \"\n",
    "                + \"connectivity matrix is [0-9]{1,2}\"\n",
    "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\"\n",
    "                + \" may not work as expected.\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, \"labels_\"):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(\n",
    "            list(\n",
    "                islice(\n",
    "                    cycle(\n",
    "                        [\n",
    "                            \"#377eb8\",\n",
    "                            \"#ff7f00\",\n",
    "                            \"#4daf4a\",\n",
    "                            \"#f781bf\",\n",
    "                            \"#a65628\",\n",
    "                            \"#984ea3\",\n",
    "                            \"#999999\",\n",
    "                            \"#e41a1c\",\n",
    "                            \"#dede00\",\n",
    "                        ]\n",
    "                    ),\n",
    "                    int(max(y_pred) + 1),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        # add black color for outliers (if any)\n",
    "        colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        # plt.xlim(-2.5, 2.5)\n",
    "        # plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(\n",
    "            0.99,\n",
    "            0.01,\n",
    "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "            transform=plt.gca().transAxes,\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_dir = '/net/projects/CLS/actomyosin_dynamics/data/decoded_results/'\n",
    "# os.mkdir(dec_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now=datetime.datetime.now()\n",
    "current_time = now.isoformat()[5:19]\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dec_dir = '/net/projects/CLS/actomyosin_dynamics/data/decoded_results/'+ model_ID+'_' + current_time\n",
    "analysis_dir = '/net/projects/CLS/actomyosin_dynamics/analysis'\n",
    "\n",
    "if not os.path.isdir(model_dec_dir):          \n",
    "    os.mkdir(model_dec_dir)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "csv_output = pd.DataFrame(columns=['title', 'labelnumber','cluster_ID'])\n",
    "for x_input, y_input in train_dataset:\n",
    "    z = model.encode(x_input)\n",
    "    x_output = model.decode(z).numpy().reshape([len(y_input), 64, 64])\n",
    "    \n",
    "    # y_pred = algorithm.predict(z.numpy())\n",
    "    for i in range(z.numpy().shape[0]):\n",
    "        input_label = y_input[i].numpy()\n",
    "        input_img = x_input[i,:,:,0].numpy().squeeze()\n",
    "        output_img = x_output[i,:,:]\n",
    "        label_str = str(int(input_label)).zfill(12)\n",
    "        ind = np.where(Label_array == input_label)        \n",
    "        pred_ID = y_pred[ind]        \n",
    "        \n",
    "        title = 't'+label_str[0:4]+'x'+label_str[4:8]+'y'+label_str[8:12]+'ps64.tif'\n",
    "        # tifffile.imsave(os.path.join(C0_dir,title), input_img)\n",
    "        tifffile.imsave(os.path.join(model_dec_dir,'dec_'+title), output_img)\n",
    "        \n",
    "        s = pd.Series([title, input_label,pred_ID],\n",
    "                           index=['title', 'labelnumber','cluster_ID'])\n",
    "        csv_output = pd.concat([csv_output, s], ignore_index=True)\n",
    "        \n",
    "\n",
    "\n",
    "csv_output.to_csv(os.path.join(analysis_dir,model_ID+'_clustering.csv'))\n",
    "                            \n",
    "        \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aetf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
